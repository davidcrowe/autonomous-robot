{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg, ObjectDetector\n",
    "\n",
    "import time\n",
    "\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import traitlets\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained coco model for use in kChaser\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the camera\n",
    "camera = Camera.instance(width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 23, 'confidence': 0.639510989189148, 'bbox': [0.3840806484222412, 0.0010841339826583862, 0.674470067024231, 0.3559338450431824]}]]\n"
     ]
    }
   ],
   "source": [
    "# detect the camera output\n",
    "# Index 1 is a human, index 18 is a dog\n",
    "detections = model(camera.value)\n",
    "print(detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5906b9f4a1cc45be888471174c872bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value=\"[[{'label': 23, 'confidence': 0.639510989189148, 'bbox': [0.3840806484222412, 0.00108413398265…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print out the detected objects\n",
    "detections_widget = widgets.Textarea()\n",
    "detections_widget.value = str(detections)\n",
    "display(detections_widget)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 23, 'confidence': 0.639510989189148, 'bbox': [0.3840806484222412, 0.0010841339826583862, 0.674470067024231, 0.3559338450431824]}\n"
     ]
    }
   ],
   "source": [
    "# print the first detected object \n",
    "image_number = 0\n",
    "object_number = 0\n",
    "print(detections[image_number][object_number])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the collision detection & balloon models\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "collision_model = torchvision.models.alexnet(pretrained=False)\n",
    "collision_model.classifier[6] = torch.nn.Linear(collision_model.classifier[6].in_features, 2)\n",
    "collision_model.load_state_dict(torch.load('../collision_avoidance/best_model.pth')) \n",
    "collision_model = collision_model.to(device)\n",
    "\n",
    "balloon_model = torchvision.models.alexnet(pretrained=False)\n",
    "balloon_model.classifier[6] = torch.nn.Linear(balloon_model.classifier[6].in_features, 2)\n",
    "balloon_model.load_state_dict(torch.load('best_balloon_model.pth'))\n",
    "balloon_model = balloon_model.to(device)\n",
    "\n",
    "\n",
    "# define the preprocessing function\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a robot instance\n",
    "robot = Robot()\n",
    "\n",
    "# confirm the robot is functional\n",
    "robot.left(.4)\n",
    "time.sleep(1.0)\n",
    "robot.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa1c8509d7e4ec0ac96bcd0f59ceabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'), FloatSlider(value=0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display all the control widgets and connect the network execution function to the camera updates\n",
    "\n",
    "blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "balloon_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='balloon')\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=18, description='tracked label')\n",
    "speed_widget = widgets.FloatSlider(value=0.4, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget, blocked_widget]),\n",
    "    balloon_widget,\n",
    "    label_widget,\n",
    "    speed_widget,\n",
    "    turn_gain_widget\n",
    "]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "\n",
    "# define object detection functions\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "        \n",
    "    \n",
    "# define the execution function to apply the neural networks frame by frame from the video feed\n",
    "def execute(change):\n",
    "    image = change['new']\n",
    "    \n",
    "    # execute collision model to determine if blocked\n",
    "    collision_output = collision_model(preprocess(image)).detach().cpu()\n",
    "    prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0])\n",
    "    blocked_widget.value = prob_blocked\n",
    "    \n",
    "    \n",
    "    # turn left if blocked\n",
    "    if prob_blocked > 0.5:\n",
    "        robot.left(0.4)\n",
    "        image_widget.value = bgr8_to_jpeg(image)\n",
    "        return\n",
    "    \n",
    "    time.sleep(0.001)\n",
    "    \n",
    "    # execute balloon to determine if a balloon is present\n",
    "    balloon_output = balloon_model(preprocess(image)).detach().cpu()\n",
    "    prob_balloon = float(F.softmax(balloon_output.flatten(), dim=0)[0])\n",
    "    balloon_widget.value = prob_balloon\n",
    "    \n",
    "    # turn right if a balloon is present\n",
    "    if prob_balloon > 0.5:\n",
    "        robot.right(.4)\n",
    "        time.sleep(1)\n",
    "        image_widget.value = bgr8_to_jpeg(image)\n",
    "        return\n",
    "    #else:\n",
    "        #robot.left(0.4)\n",
    "        \n",
    "    time.sleep(0.001)\n",
    "        \n",
    "    # compute all detected objects\n",
    "    detections = model(image)\n",
    "    \n",
    "    # draw all detections on image\n",
    "    for det in detections[0]:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "    \n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections)\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "    \n",
    "        \n",
    "    # otherwise go forward if no target detected\n",
    "    if det is None:\n",
    "        robot.forward(float(speed_widget.value))\n",
    "        \n",
    "    # otherwsie steer towards target\n",
    "    else:\n",
    "        # move robot forward and steer proportional target's x-distance from center\n",
    "        center = detection_center(det)\n",
    "        robot.set_motors(\n",
    "            float(speed_widget.value - turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value + turn_gain_widget.value * center[0])\n",
    "        )\n",
    "    \n",
    "    # update image widget\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect the execute function to each camera frame update\n",
    "\n",
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually disconnect the processing from the camera and stop the robot\n",
    "\n",
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and load the trained model \n",
    "\n",
    "#model = torchvision.models.resnet18(pretrained=False)\n",
    "#model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)\n",
    "\n",
    "\n",
    "#resnet18 = models.resnet18()\n",
    "\n",
    "# change the model name below as necessary\n",
    "#model.load_state_dict(torch.load('models_ft_state_dict.pth'))\n",
    "\n",
    "#model = torch.load(models_ft.pth)\n",
    "\n",
    "model = torch.load(\"models_ft.pth\")\n",
    "\n",
    "# execute the code on the gpu vs. cpu\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pre-processing function\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start and display the camera\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "blocked_slider = widgets.FloatSlider(description='blocked', min=0.0, max=1.0, orientation='vertical')\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(widgets.HBox([image, blocked_slider]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create robot instance to drive the motors\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the robot is functioning\n",
    "\n",
    "robot.forward(speed=0.4)\n",
    "time.sleep(1)\n",
    "robot.stop()\n",
    "\n",
    "robot.left(speed=0.4)\n",
    "time.sleep(1)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the neural network execution function\n",
    "# if probability that kenai is in the photo is > 0.5, robot will turn\n",
    "\n",
    "def update(change):\n",
    "    global blocked_slider, robot\n",
    "    x = change['new'] \n",
    "    x = preprocess(x)\n",
    "    y = model(x)\n",
    "    \n",
    "    # we apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
    "    y = F.softmax(y, dim=1)\n",
    "    \n",
    "    prob_blocked = float(y.flatten()[0])\n",
    "    \n",
    "    blocked_slider.value = prob_blocked\n",
    "    \n",
    "    if prob_blocked < 0.5:\n",
    "        robot.backward(0.4)\n",
    "    else:\n",
    "        robot.left(0.4)\n",
    "    \n",
    "    time.sleep(0.001)\n",
    "        \n",
    "update({'new': camera.value})  # we call the function once to intialize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the neural network execution function to the camera for processing \n",
    "\n",
    "camera.observe(update, names='value')  # this attaches the 'update' function to the 'value' traitlet of our camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the robot\n",
    "\n",
    "camera.unobserve(update, names='value')\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlink & link the camera feed as needed\n",
    "\n",
    "camera_link.unlink()  # don't stream to browser (will still run camera)\n",
    "camera_link.link()  # stream to browser (wont run camera)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and display widgets\n",
    "# widgets include sliders, buttons, video feed, etc. \n",
    "\n",
    "# create a stop button! \n",
    "# create a display that highlights identified object in green\n",
    "# slider for speed\n",
    "# slider for probability that target objects are identified\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect the widgets to the robot with traitlets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlink the trailets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
